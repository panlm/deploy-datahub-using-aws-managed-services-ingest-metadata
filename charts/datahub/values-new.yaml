# Values to start up datahub after starting up the datahub-prerequisites chart with "prerequisites" release name
# Copy this chart and change configuration as needed.
datahub-gms:
  enabled: true

datahub-frontend:
  enabled: true

acryl-datahub-actions:
  enabled: true
  resources:
    limits:
      memory: 4Gi
    requests:
      cpu: 300m
      memory: 1Gi

elasticsearchSetupJob:
  enabled: true
  # If you want to use OpenSearch instead of ElasticSearch add the USE_AWS_ELASTICSEARCH environment variable below
  extraEnvs:
    - name: USE_AWS_ELASTICSEARCH
      value: "true"

kafkaSetupJob:
  enabled: true

mysqlSetupJob:
  enabled: true

## No code data migration
datahubUpgrade:
  enabled: true

## Runs system update processes
## Includes: Elasticsearch Indices Creation/Reindex (See global.elasticsearch.index for additional configuration)
datahubSystemUpdate:
  enabled: true

global:
  strict_mode: true
  graph_service_impl: elasticsearch
  datahub_analytics_enabled: true
  datahub_standalone_consumers_enabled: false

  elasticsearch:
    host: "vpc-opensearch-domain-xxx.region.es.amazonaws.com"
    # If you want to use OpenSearch instead of ElasticSearch use different hostname below
    # host: "opensearch-cluster-master"
    port: "443"
    skipcheck: "false"
    insecure: "false"
    useSSL: "true"
    # If you want to specify index prefixes use indexPrefix
    # indexPrefix: "dh"
    auth:
      username: opensearch
      password:
        secretRef: elasticsearch-secrets
        secretKey: elasticsearch-password

  kafka:
    bootstrap:
      # server: "prerequisites-kafka:9092"
      server: "b-1.xxx.kafka.region.amazonaws.com:9092,b-2.xxx.kafka.region.amazonaws.com:9092"
    zookeeper:
      # server: "prerequisites-zookeeper:2181"
      server: "z-1.xxx.kafka.region.amazonaws.com:2181,z-3.xxx.kafka.region.amazonaws.com:2181,z-2.xxx.kafka.region.amazonaws.com:2181"
    ## For AWS MSK set this to a number larger than 1
    partitions: 2
    replicationFactor: 3
    schemaregistry:
      # GMS Implementation - `url` configured based on component context
      type: INTERNAL
      # Confluent Kafka Implementation
      # type: KAFKA
      url: "http://prerequisites-cp-schema-registry:8081"

      # Glue Implementation - `url` not applicable
      # type: AWS_GLUE
      # glue:
      #   region: us-east-1
      #   registry: datahub

  # neo4j:
  #   host: "prerequisites-neo4j:7474"
  #   uri: "bolt://prerequisites-neo4j"
  #   username: "neo4j"
  #   password:
  #     secretRef: neo4j-secrets
  #     secretKey: neo4j-password
  #   # --------------OR----------------
  #   # value: password

  sql:
    datasource:
      host: "mysql-xxx.region.rds.amazonaws.com:3306"
      hostForMysqlClient: "mysql-xxx.region.rds.amazonaws.com"
      port: "3306"
      url: "jdbc:mysql://mysql-xxx.region.rds.amazonaws.com:3306/datahub?verifyServerCertificate=false&useSSL=true&useUnicode=yes&characterEncoding=UTF-8&enabledTLSProtocols=TLSv1.2"
      driver: "com.mysql.cj.jdbc.Driver"
      username: "admin"
      password:
        secretRef: mysql-secrets
        secretKey: mysql-root-password

  datahub:
    version: v0.13.2
    gms:
      protocol: "http"
      port: "8080"
      nodePort: "30001"

    frontend:
      validateSignUpEmail: true

    monitoring:
      enablePrometheus: true
      # Set a custom name for the monitoring port
      portName: jmx

    mae_consumer:
      port: "9091"
      nodePort: "30002"

    appVersion: "1.0"
    systemUpdate:
      ## The following options control settings for datahub-upgrade job which will
      ## managed ES indices and other update related work
      enabled: true

    encryptionKey:
      secretRef: "datahub-encryption-secrets"
      secretKey: "encryption_key_secret"
      # Set to false if you'd like to provide your own secret.
      provisionSecret:
        enabled: true
        autoGenerate: true
        annotations: {}
      # Only specify if autoGenerate set to false
      #  secretValues:
      #    encryptionKey: <encryption key value>

    managed_ingestion:
      enabled: true
      defaultCliVersion: "0.13.1.2"

    metadata_service_authentication:
      enabled: true
      systemClientId: "__datahub_system"
      systemClientSecret:
        secretRef: "datahub-auth-secrets"
        secretKey: "system_client_secret"
      tokenService:
        signingKey:
          secretRef: "datahub-auth-secrets"
          secretKey: "token_service_signing_key"
        salt:
          secretRef: "datahub-auth-secrets"
          secretKey: "token_service_salt"
      # Set to false if you'd like to provide your own auth secrets
      provisionSecrets:
        enabled: true
        autoGenerate: true
        annotations: {}
      # Only specify if autoGenerate set to false
      #  secretValues:
      #    secret: <secret value>
      #    signingKey: <signing key value>
      #    salt: <salt value>

    ## Enables always emitting a MCL even when no changes are detected. Used for Time Based Lineage when no changes occur.
    alwaysEmitChangeLog: false

    ## Enables diff mode for graph writes, uses a different code path that produces a diff from previous to next to write relationships instead of wholesale deleting edges and reading
    enableGraphDiffMode: true

    ## Values specific to the unified search and browse feature.
    search_and_browse:
      show_search_v2: true  # If on, show the new search filters experience as of v0.10.5
      show_browse_v2: true  # If on, show the new browse experience as of v0.10.5
      backfill_browse_v2: true  # If on, run the backfill upgrade job that generates default browse paths for relevant entities

    ## v0.13.4+
    mcp:
      throttle:
        # updateIntervalMs: 60000
        ## Versioned MCL topic
        versioned:
          ## Whether to throttle MCP processing based on MCL backlog
          enabled: true
        #  threshold: 4000
        #  maxAttempts: 1000
        #  initialIntervalMs: 100
        #  multiplier: 10
        #  maxIntervalMs:  30000
        # Timeseries MCL topic
        timeseries:
          ## Whether to throttle MCP processing based on MCL backlog
          enabled: false
        #  threshold: 4000
        #  maxAttempts: 1000
        #  initialIntervalMs: 100
        #  multiplier: 10
        #  maxIntervalMs: 30000

#  hostAliases:
#    - ip: "192.168.0.104"
#      hostnames:
#        - "broker"
#        - "mysql"
#        - "postgresql"
#        - "elasticsearch"
#        - "neo4j"

## Add below to enable SSL for kafka
#  credentialsAndCertsSecrets:
#    name: datahub-certs
#    path: /mnt/datahub/certs
#    secureEnv:
#      ssl.key.password: datahub.linkedin.com.KeyPass
#      ssl.keystore.password: datahub.linkedin.com.KeyStorePass
#      ssl.truststore.password: datahub.linkedin.com.TrustStorePass
#      kafkastore.ssl.truststore.password: datahub.linkedin.com.TrustStorePass
#
#  springKafkaConfigurationOverrides:
#    ssl.keystore.location: /mnt/datahub/certs/datahub.linkedin.com.keystore.jks
#    ssl.truststore.location: /mnt/datahub/certs/datahub.linkedin.com.truststore.jks
#    kafkastore.ssl.truststore.location: /mnt/datahub/certs/datahub.linkedin.com.truststore.jks
#    security.protocol: SSL
#    kafkastore.security.protocol: SSL
#    ssl.keystore.type: JKS
#    ssl.truststore.type: JKS
#    ssl.protocol: TLS
#    ssl.endpoint.identification.algorithm:
#    basic.auth.credentials.source: USER_INFO
#    basic.auth.user.info:

